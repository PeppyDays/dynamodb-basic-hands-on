{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Items in Batch\n",
    "\n",
    "DynamoDB also provides batch operations.\n",
    "\n",
    "- `BatchGetItem` reads up to 100 items from one or more tables\n",
    "- `BatchWriteItem` creates or deletes up to 25 items in one or more tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchWriteItem\n",
    "\n",
    "There are two ways to do `BatchWriteItem` operation in Python. Basically the two ways call the same AWS API.\n",
    "\n",
    "- Use DynamoDB client batch_write_item method\n",
    "    - Limited up to 25 put or delete requests in a single batch\n",
    "    - Each operation is atomic, but the batch as a whole is not\n",
    "        - Check `UnprocessedItems` in a response\n",
    "- Use DynamoDB resource table batch_writer method\n",
    "    - Automatically handles buffering and sending items in batches\n",
    "    - Automatically handle any unprocessed items and resend them as needed\n",
    "    - No response returned about the result\n",
    "\n",
    "Let's try to import `data/starbucks.csv` by using `batch_writer` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and get dynamodb resource\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "from botocore.exceptions import ClientError\n",
    "from pprint import pprint, pformat\n",
    "from decimal import Decimal\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "starbucks = dynamodb.Table('Starbucks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in items list is 25600.\n",
      "Here is a sample data: \n",
      "{'Brand': 'Starbucks',\n",
      " 'City': 'Andorra la Vella',\n",
      " 'Country': 'AD',\n",
      " 'Latitude': Decimal('42.51'),\n",
      " 'Longitude': Decimal('1.53'),\n",
      " 'OwnershipType': 'Licensed',\n",
      " 'PhoneNumber': '376818720',\n",
      " 'Postcode': 'AD500',\n",
      " 'State': '7',\n",
      " 'StateCity': '7::Andorra la Vella',\n",
      " 'StoreName': 'Meritxell, 96',\n",
      " 'StoreNumber': '47370-257954',\n",
      " 'StreetAddress': 'Av. Meritxell, 96',\n",
      " 'Timezone': 'GMT+1:00 Europe/Andorra'}\n"
     ]
    }
   ],
   "source": [
    "# cleanse raw data\n",
    "items = []\n",
    "\n",
    "with open('data/starbucks.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(\n",
    "        f,\n",
    "        fieldnames=['Brand', 'StoreNumber', 'StoreName', 'OwnershipType', 'StreetAddress', 'City', 'State', 'Country', 'Postcode', 'PhoneNumber', 'Timezone', 'Longitude', 'Latitude']\n",
    "    )\n",
    "    next(reader, None)\n",
    "    \n",
    "    for row in reader:\n",
    "        item = {key: value for key, value in row.items() if value != ''}\n",
    "        try:\n",
    "            item['Longitude'] = Decimal(item['Longitude'])\n",
    "            item['Latitude'] = Decimal(item['Latitude'])\n",
    "            item['StateCity'] = item['State'] + '::' + item['City']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        items.append(item)\n",
    "        \n",
    "print('Total rows in items list is {}.'.format(len(items)))\n",
    "print('Here is a sample data: \\n{}'.format(pformat(items[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_batch_write is done in 932.39 ms\n",
      "table_batch_write is done in 927.25 ms\n",
      "table_batch_write is done in 991.20 ms\n",
      "table_batch_write is done in 1032.37 ms\n",
      "table_batch_write is done in 1011.40 ms\n",
      "table_batch_write is done in 1089.11 ms\n",
      "table_batch_write is done in 1208.03 ms\n",
      "table_batch_write is done in 1274.58 ms\n",
      "table_batch_write is done in 1647.18 ms\n",
      "table_batch_write is done in 1905.23 ms\n",
      "table_batch_write is done in 1706.88 ms\n",
      "table_batch_write is done in 2002.59 ms\n",
      "table_batch_write is done in 2002.82 ms\n",
      "table_batch_write is done in 2164.59 ms\n",
      "table_batch_write is done in 2211.30 ms\n",
      "table_batch_write is done in 1961.16 ms\n",
      "table_batch_write is done in 1989.15 ms\n",
      "table_batch_write is done in 1973.75 ms\n",
      "table_batch_write is done in 1968.24 ms\n",
      "table_batch_write is done in 2332.76 ms\n",
      "table_batch_write is done in 1947.73 ms\n",
      "table_batch_write is done in 1838.47 ms\n",
      "table_batch_write is done in 2162.88 ms\n",
      "table_batch_write is done in 2024.52 ms\n",
      "table_batch_write is done in 540.33 ms\n",
      "table_batch_write is done in 873.01 ms\n",
      "all is done in 5958.21 ms\n"
     ]
    }
   ],
   "source": [
    "# batch put items in parallel\n",
    "def group_items(iterator, n=25):\n",
    "    \"\"\"\n",
    "    Split input list to the sub-lists with given number\n",
    "    \"\"\"\n",
    "    accumulator = []\n",
    "    \n",
    "    for item in iterator:\n",
    "        accumulator.append(item)\n",
    "        if len(accumulator) == n:\n",
    "            yield accumulator\n",
    "            accumulator = []\n",
    "            \n",
    "    if accumulator:\n",
    "        yield accumulator\n",
    "\n",
    "def table_batch_write(table, items):\n",
    "    \"\"\"\n",
    "    Batch writer with Table's batch_writer() context\n",
    "    \"\"\"\n",
    "    ts = time.time()\n",
    "    \n",
    "    with boto3.resource('dynamodb').Table(table).batch_writer() as batch:\n",
    "        for item in items:\n",
    "            batch.put_item(Item=item)\n",
    "    \n",
    "    te = time.time()\n",
    "    duration_ms = (te - ts) * 1000\n",
    "    print('table_batch_write is done in {:.2f} ms'.format(duration_ms))\n",
    "\n",
    "\n",
    "table = 'Starbucks'\n",
    "batch_size = 1000\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    pool.starmap(table_batch_write, [(table, grouped_items) for grouped_items in group_items(items, batch_size)])\n",
    "    pool.close()\n",
    "\n",
    "te = time.time()\n",
    "duration_ms = (te - ts) * 1000\n",
    "print('all is done in {:.2f} ms'.format(duration_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchGetItem\n",
    "\n",
    "The `BatchGetItem` operation returns the attributes of one or more items from one or more tables. You identify requested items by primary key.\n",
    "\n",
    "A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. If a partial result is returned, the operation returns a value for `UnprocessedKeys`.\n",
    "\n",
    "By default, BatchGetItem performs eventually consistent reads on every table in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ConsumedCapacity': [{'CapacityUnits': 1.0,\n",
      "                       'Table': {'CapacityUnits': 1.0},\n",
      "                       'TableName': 'Starbucks'}],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
      "                                      'content-length': '540',\n",
      "                                      'content-type': 'application/x-amz-json-1.0',\n",
      "                                      'date': 'Sun, 04 Oct 2020 19:37:22 GMT',\n",
      "                                      'server': 'Server',\n",
      "                                      'x-amz-crc32': '2673786153',\n",
      "                                      'x-amzn-requestid': 'J3H470A1JMSG6MQFMENE3NOE4RVV4KQNSO5AEMVJF66Q9ASUAAJG'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'J3H470A1JMSG6MQFMENE3NOE4RVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
      "                      'RetryAttempts': 0},\n",
      " 'Responses': {'Starbucks': [{'Brand': 'Starbucks',\n",
      "                              'City': 'Andorra la Vella',\n",
      "                              'Country': 'AD',\n",
      "                              'Latitude': Decimal('42.51'),\n",
      "                              'Longitude': Decimal('1.53'),\n",
      "                              'OwnershipType': 'Licensed',\n",
      "                              'PhoneNumber': '376818720',\n",
      "                              'Postcode': 'AD500',\n",
      "                              'State': '7',\n",
      "                              'StoreName': 'Meritxell, 96',\n",
      "                              'StoreNumber': '47370-257954',\n",
      "                              'StreetAddress': 'Av. Meritxell, 96',\n",
      "                              'Timezone': 'GMT+1:00 Europe/Andorra'}]},\n",
      " 'UnprocessedKeys': {}}\n"
     ]
    }
   ],
   "source": [
    "response = dynamodb.batch_get_item(\n",
    "    RequestItems={\n",
    "        'Starbucks': {\n",
    "            'Keys': [\n",
    "                {\n",
    "                    'StoreNumber': '47370-257954'\n",
    "                },\n",
    "                {\n",
    "                    'StoreNumber': '47370-257955'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    ReturnConsumedCapacity='INDEXES'\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Operations and Error Handling\n",
    "\n",
    "A batch operation can tolerate the failure of individual requests in the batch. For example, consider a `BatchGetItem` request to read five items. Even if some of the underlying `GetItem` requests fail, this does not cause the entire `BatchGetItem` operation to fail. However, if all five read operations fail, then the entire `BatchGetItem` fails.\n",
    "\n",
    "The batch operations return information about individual requests that fail so that you can diagnose the problem and retry the operation. For `BatchGetItem`, the tables and primary keys in question are returned in the `UnprocessedKeys` value of the response. For `BatchWriteItem`, similar information is returned in `UnprocessedItems`.\n",
    "\n",
    "The most likely cause of a failed read or a failed write is throttling. If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. However, we strongly recommend that you use an exponential backoff algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
